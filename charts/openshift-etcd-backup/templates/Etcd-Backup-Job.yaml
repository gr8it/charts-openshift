---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: {{ include "app.fullname" . }}
  namespace: {{ .Values.defaultNamespace }}
  labels:
    {{- include "common.labels" . | nindent 4 }}
spec:
  schedule: {{ .Values.etcdBackupSchedule | quote }}
  concurrencyPolicy: Forbid
  suspend: false
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 7
  jobTemplate:
    metadata:
      labels:
        {{- include "common.labels" . | nindent 8 }}
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            {{- include "common.labels" . | nindent 12 }}
        spec:
          nodeSelector:
            node-role.kubernetes.io/master: ''
          restartPolicy: Never
          activeDeadlineSeconds: 900
          serviceAccountName: {{ include "app.fullname" . }}
          hostPID: true
          hostNetwork: true
          dnsPolicy: ClusterFirstWithHostNet
          enableServiceLinks: true
          schedulerName: default-scheduler
          terminationGracePeriodSeconds: 30
          securityContext: {}

          # Init container for Snapshot & Cleanup
          initContainers:
          - name: etcd-snapshot
            image: {{ .Values.image.busybox }}
            imagePullPolicy: Always
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            command:
            - /bin/sh
            - '-c'
            - |
              set -euo pipefail
              # cleanup
              echo -e "\n\n---\nCleanup old local etcd snapshots on $NODENAME\n"
              find /host/home/core/backup -mmin +60 -path '*/backup/*' -delete -print
              # snapshot
              echo -e "\n\n---\nCreate new local etcd snapshot on $NODENAME\n"
              TIMESTAMP=$(date -u +"%F_%H%M%SZ")
              chroot /host /usr/local/bin/cluster-backup.sh /home/core/backup/${TIMESTAMP}
              {{- if and (hasKey .Values "compressSnapshot") ( eq (default "false" (toString .Values.compressSnapshot)) "true" ) }}
              # archive
              echo -e "\n\n---\nCompress the etcd snapshot\n"
              gzip -9v /host/home/core/backup/${TIMESTAMP}/snapshot*
              gzip -tv /host/home/core/backup/${TIMESTAMP}/snapshot*
              {{- end }}
              exit 0
            env:
            - name: NODENAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            securityContext:
              privileged: true
              runAsUser: 0
              capabilities:
                add: ["SYS_CHROOT"]
            volumeMounts:
            - name: host
              mountPath: /host
            resources:
              requests: { cpu: "200m", memory: "512Mi" }
              limits: { cpu: "500m", memory: "1Gi"}

          # Main container for AWS S3 upload
          containers:
          - name: s3-upload
            image: {{ .Values.image.awscli }}
            command:
            - /bin/bash
            - '-c'
            - |
              set -euo pipefail

              export AWS_ENDPOINT_URL="https://${BUCKET_HOST}:${BUCKET_PORT}"

              # check backup folder
              SNAPSHOT=$(find /host/home/core/backup -mindepth 1 -maxdepth 1 -type d -mmin -30 | sort | tail -1)
              if ! test -d "$SNAPSHOT"; then
                echo "No new valid snapshot folder found to upload. Exiting gracefully."
                exit 1
              fi
              # s3 upload
              echo "Uploading etcd snapshot to s3://${BUCKET_NAME} "
              if ! aws s3 cp "$SNAPSHOT" "s3://${BUCKET_NAME}/$(basename $SNAPSHOT)" --recursive --no-progress; then
                echo "Failed to upload snapshot folder to S3. Exiting gracefully."
                exit 2
              fi
              # cleanup
              if ! expr "$RETENTION" : '^[0-9]\+$' >/dev/null; then
                echo "Value for retention is not a valid integer. Skipping cleanup."
                exit 3
              fi
              if ! RETENTION_DATE=$( date -u --date="${RETENTION} days ago" +"%FT%H:%M:%SZ" ); then
                echo "Failed to set a retention date. Skipping cleanup."
                exit 3
              fi
              if ! aws s3api list-objects-v2 --bucket $BUCKET_NAME --query "Contents[?LastModified<='$RETENTION_DATE'].{Key: Key} | {Objects: [*]}" 2>/dev/null 1>/dev/shm/delete.json; then
                echo "Failed to generate a list of objects to delete. Skipping cleanup."
                exit 3
              fi
              if grep -qE '"Objects":\s*\[\s*\]' /dev/shm/delete.json; then
                echo "No objects older than $RETENTION days found. Skipping cleanup."
                exit 0
              fi

              echo "Removing objects older than $RETENTION days:"
              if ! aws s3api delete-objects --bucket $BUCKET_NAME --delete file:///dev/shm/delete.json --output text ; then
                echo "Failed to remove objects."
                exit 3
              fi

              exit 0
            envFrom:
            - secretRef:
                name: {{ include "obc.name" . }}
            - configMapRef:
                name: {{ include "obc.name" . }}
            env:
            - name: RETENTION
              value: {{ default 30 .Values.retentionDays | quote }}
            - name: AWS_CA_BUNDLE
              value: "/etc/ssl/certs/service-ca.crt"
            - name: AWS_REQUEST_CHECKSUM_CALCULATION
              value: when_required
            - name: AWS_RESPONSE_CHECKSUM_VALIDATION
              value: when_required
            volumeMounts:
            - name: host
              mountPath: /host
            - name: service-ca
              mountPath: /etc/ssl/certs/service-ca.crt
              subPath: service-ca.crt
              readOnly: true
            resources:
              requests: { cpu: "50m", memory: "100Mi" }
              limits: { cpu: "500m", memory: "1Gi" }
          volumes:
          - name: host
            hostPath:
              path: /
              type: Directory
          - name: service-ca
            configMap:
              name: openshift-service-ca.crt
          tolerations:
          - key: node-role.kubernetes.io/master
            operator: "Exists"
            effect: "NoSchedule"
